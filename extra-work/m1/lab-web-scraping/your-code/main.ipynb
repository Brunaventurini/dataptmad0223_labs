{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "html = response.content\n",
    "html[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_html = bs4.BeautifulSoup(html[:500], \"html.parser\") \n",
    "print(parsed_html.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [tag.a in soup.find_all('h1,{class:[h3, lh-condensed]})\n",
    "names = [tag.string.strip() for tag in tags if tag.string != None]\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://api.github.com/search/repositories'\n",
    "params = {\n",
    "    'q': 'language:python',\n",
    "    'sort': 'stars',\n",
    "    'order': 'desc',\n",
    "    'per_page': 10\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# Extract repository names from the response\n",
    "repo_names = [item['full_name'] for item in data['items']]\n",
    "\n",
    "# Display the repository names\n",
    "for name in repo_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object with the response text\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the image elements\n",
    "image_elements = soup.find_all('img')\n",
    "\n",
    "# Extract the source (src) attribute from each image element\n",
    "image_links = [image['src'] for image in image_elements]\n",
    "\n",
    "# Display the image links\n",
    "for link in image_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Python'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object with the response text\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the anchor elements\n",
    "anchor_elements = soup.find_all('a')\n",
    "\n",
    "# Extract the href attribute from each anchor element\n",
    "links = [anchor.get('href') for anchor in anchor_elements]\n",
    "\n",
    "# Filter out None values and links that don't start with '/wiki/'\n",
    "links = [link for link in links if link and link.startswith('/wiki/')]\n",
    "\n",
    "# Display the links\n",
    "for link in links:\n",
    "    print(link)\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the number of titles that have changed in the United States Code since its last release point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object with the response text\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the download links for the United States Code\n",
    "download_links = soup.find_all('a', href=lambda href: href and href.startswith('/download/'))\n",
    "\n",
    "# Extract the count of changed titles from the number of download links\n",
    "changed_titles_count = len(download_links)\n",
    "\n",
    "# Display the count of changed titles\n",
    "print(f\"The number of titles changed in the United States Code since its last release point: {changed_titles_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a Python list with the top ten FBI's Most Wanted names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object with the response text\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the list items (li) containing the wanted names\n",
    "wanted_names = soup.find_all('li', class_='portal-type-person castle-grid-block-item')\n",
    "\n",
    "# Extract the names from the list items\n",
    "top_ten_wanted_names = [name.text.strip() for name in wanted_names]\n",
    "\n",
    "# Display the top ten wanted names\n",
    "print(\"Top Ten FBI's Most Wanted Names:\")\n",
    "for name in top_ten_wanted_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object with the response text\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the container that holds the earthquake information\n",
    "container = soup.find('div', id='tbody')\n",
    "\n",
    "# Find all the rows in the container\n",
    "rows = container.find_all('tr')\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Extract the information from each row\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    if len(cols) == 13:  # Check for valid row\n",
    "        date = cols[3].text.strip()\n",
    "        time = cols[4].text.strip()\n",
    "        latitude = cols[5].text.strip()\n",
    "        longitude = cols[6].text.strip()\n",
    "        region = cols[11].text.strip()\n",
    "        data.append([date, time, latitude, longitude, region])\n",
    "\n",
    "# Create a Pandas DataFrame with the extracted data\n",
    "columns = ['Date', 'Time', 'Latitude', 'Longitude', 'Region']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of tweets by a given Twitter account.\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# Set up your Twitter API credentials\n",
    "consumer_key = 'YOUR_CONSUMER_KEY'\n",
    "consumer_secret = 'YOUR_CONSUMER_SECRET'\n",
    "access_token = 'YOUR_ACCESS_TOKEN'\n",
    "access_token_secret = 'YOUR_ACCESS_TOKEN_SECRET'\n",
    "\n",
    "# Authenticate with the Twitter API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "try:\n",
    "    # Create the API object\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    # Ask the user for the Twitter handle\n",
    "    handle = input(\"Enter the Twitter handle (without '@') of the account: \")\n",
    "\n",
    "    # Get the user object for the given handle\n",
    "    user = api.get_user(screen_name=handle)\n",
    "\n",
    "    # Get the tweet count from the user object\n",
    "    tweet_count = user.statuses_count\n",
    "\n",
    "    # Display the tweet count\n",
    "    print(f\"The number of tweets by @{handle}: {tweet_count}\")\n",
    "\n",
    "except tweepy.error.TweepError as e:\n",
    "    if e.api_code == 50:\n",
    "        print(f\"Account '@{handle}' not found.\")\n",
    "    else:\n",
    "        print(f\"An error occurred while accessing the Twitter API: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "consumer_key = 'YOUR_CONSUMER_KEY'\n",
    "consumer_secret = 'YOUR_CONSUMER_SECRET'\n",
    "access_token = 'YOUR_ACCESS_TOKEN'\n",
    "access_token_secret = 'YOUR_ACCESS_TOKEN_SECRET'\n",
    "\n",
    "handle = input(\"Enter the Twitter handle (without '@') of the account: \")\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "try:\n",
    "    user = api.get_user(screen_name=handle)\n",
    "    follower_count = user.followers_count\n",
    "    print(f\"The follower count for @{handle} is: {follower_count}\")\n",
    "except tweepy.error.TweepError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the Wikipedia homepage\n",
    "url = 'https://www.wikipedia.org/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the container that holds the language links\n",
    "language_container = soup.find('div', class_='central-featured-lang')\n",
    "\n",
    "# Find all the language links within the container\n",
    "language_links = language_container.find_all('a')\n",
    "\n",
    "# Iterate over the language links and extract the language name and the number of related articles\n",
    "for link in language_links:\n",
    "    language_name = link.text.strip()\n",
    "    article_count = link.find_next('bdi').text.strip()\n",
    "    print(f\"Language: {language_name}\\nNumber of articles: {article_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m categories_container \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnav\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdguk-secondary-navigation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Get all the child elements of the container\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m category_elements \u001b[38;5;241m=\u001b[39m \u001b[43mcategories_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;28;01mTrue\u001b[39;00m, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Create an empty list to store the dataset categories\u001b[39;00m\n\u001b[0;32m     18\u001b[0m dataset_categories \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the data.gov.uk homepage\n",
    "url = 'https://data.gov.uk/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the parent container that holds the dataset categories\n",
    "categories_container = soup.find('nav', class_='dguk-secondary-navigation')\n",
    "\n",
    "# Get all the child elements of the container\n",
    "category_elements = categories_container.find_all(True, recursive=False)\n",
    "\n",
    "# Create an empty list to store the dataset categories\n",
    "dataset_categories = []\n",
    "\n",
    "# Iterate over the child elements and extract the category names\n",
    "for element in category_elements:\n",
    "    if element.name == 'a':\n",
    "        category_name = element.get_text(strip=True)\n",
    "        dataset_categories.append(category_name)\n",
    "\n",
    "# Print the list of dataset categories\n",
    "for category in dataset_categories:\n",
    "    print(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the top 10 languages by number of native speakers stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language  Native Speakers\n",
      "0      939              NaN\n",
      "1      485              NaN\n",
      "2      380              NaN\n",
      "3      345              NaN\n",
      "4      236              NaN\n",
      "5      234              NaN\n",
      "6      147              NaN\n",
      "7      123              NaN\n",
      "8     86.1              NaN\n",
      "9     85.0              NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table that contains the language data\n",
    "table = soup.find('table', class_='wikitable')\n",
    "\n",
    "# Initialize empty lists to store the language data\n",
    "languages = []\n",
    "native_speakers = []\n",
    "\n",
    "# Iterate over the rows in the table (excluding the header row)\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    # Get the columns for each row\n",
    "    columns = row.find_all('td')\n",
    "    # Extract the language and native speaker count from the columns\n",
    "    language = columns[1].text.strip()\n",
    "    speakers = columns[2].text.strip().replace(',', '')\n",
    "    # Append the language and speaker count to the respective lists\n",
    "    languages.append(language)\n",
    "    native_speakers.append(speakers)\n",
    "\n",
    "# Create a pandas dataframe with the language data\n",
    "data = {'Language': languages, 'Native Speakers': native_speakers}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the 'Native Speakers' column to numeric values\n",
    "df['Native Speakers'] = pd.to_numeric(df['Native Speakers'], errors='coerce')\n",
    "\n",
    "# Sort the dataframe by the number of native speakers in descending order\n",
    "df = df.sort_values(by='Native Speakers', ascending=False)\n",
    "\n",
    "# Select the top 10 languages\n",
    "top_10_languages = df.head(10)\n",
    "\n",
    "# Display the top 10 languages dataframe\n",
    "print(top_10_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Movie Name Initial Release Director Stars\n",
      "0           1.\\n      Cadena perpetua\\n(1994)            1994      N/A    []\n",
      "1                2.\\n      El padrino\\n(1972)            1972      N/A    []\n",
      "2       3.\\n      El caballero oscuro\\n(2008)            2008      N/A    []\n",
      "3     4.\\n      El padrino (parte II)\\n(1974)            1974      N/A    []\n",
      "4     5.\\n      12 hombres sin piedad\\n(1957)            1957      N/A    []\n",
      "..                                        ...             ...      ...   ...\n",
      "245     246.\\n      Criadas y señoras\\n(2011)            2011      N/A    []\n",
      "246      247.\\n      La vida de Brian\\n(1979)            1979      N/A    []\n",
      "247  248.\\n      El gigante de hierro\\n(1999)            1999      N/A    []\n",
      "248               249.\\n      Aladdín\\n(1992)            1992      N/A    []\n",
      "249              250.\\n      Drishyam\\n(2015)            2015      N/A    []\n",
      "\n",
      "[250 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the IMDB top 250 page\n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table that holds the movie data\n",
    "table = soup.find('table', class_='chart')\n",
    "\n",
    "# Find all the rows in the table body\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "movie_names = []\n",
    "initial_releases = []\n",
    "directors = []\n",
    "stars = []\n",
    "\n",
    "# Iterate over the rows, skipping the header row\n",
    "for row in rows[1:]:\n",
    "    # Find the columns in the row\n",
    "    columns = row.find_all('td')\n",
    "\n",
    "    # Extract the movie name, initial release, director, and stars from the columns\n",
    "    movie_name = columns[1].text.strip()\n",
    "    initial_release = columns[1].find('span', class_='secondaryInfo').text.strip('()')\n",
    "\n",
    "    director_element = columns[3].find('a')\n",
    "    if director_element is not None:\n",
    "        director = director_element.text.strip()\n",
    "    else:\n",
    "        director = 'N/A'\n",
    "\n",
    "    star_elements = columns[2].find_all('a')\n",
    "    star_names = [star.text.strip() for star in star_elements]\n",
    "\n",
    "    # Append the data to the respective lists\n",
    "    movie_names.append(movie_name)\n",
    "    initial_releases.append(initial_release)\n",
    "    directors.append(director)\n",
    "    stars.append(star_names)\n",
    "\n",
    "# Create a pandas dataframe with the scraped data\n",
    "data = {\n",
    "    'Movie Name': movie_names,\n",
    "    'Initial Release': initial_releases,\n",
    "    'Director': directors,\n",
    "    'Stars': stars\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:m1_env]",
   "language": "python",
   "name": "conda-env-m1_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
